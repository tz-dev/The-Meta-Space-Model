# Script: 11_2mass_psc_validator.py
# Description: Modular validator for 2MASS PSC files (EP6: Dark Matter Projection) focusing on ASCII data analysis.
#              Extended to include source density analysis for structural validation.
# Author: Automatically generated by Grok 3 (xAI)
# Date: 2025-07-07
# Version: 1.0
# Inputs: config_2mass.json (configuration file), psc_aaa to psc_aal (2MASS PSC ASCII files)
# Outputs: results.csv (log of analysis results), img/11_source_density_heatmap.png (histogram heatmap),
#          z_sky_mean.csv (sky-binned source density data), z_sky_mean_map.png (sky map visualization)
# Dependencies: numpy (numerical operations), matplotlib (plotting), csv (file I/O), json (configuration parsing),
#               os (file system operations), logging (error logging), tabulate (table formatting),
#               subprocess (script execution), tqdm (progress bars), cupy (GPU acceleration, optional)
# Purpose: Validates source density projections by analyzing 2MASS PSC data, performing sky binning,
#          generating visualizations, and estimating structural consistency.
#          Supports CUDA acceleration if available.
#
# About config_2mass.json:
#    Based on the expected source density from 2MASS PSC (~1 source per square arcminute on average),
#    the threshold of ±0.5 sources/arcmin² allows for local structure variation while preserving sensitivity.

import numpy as np  # Import NumPy for numerical computations
import matplotlib.pyplot as plt  # Import Matplotlib for plotting
import csv, json, os, logging, sys, time, subprocess  # Import various standard libraries
from datetime import datetime  # Import for timestamp generation
from tabulate import tabulate  # Import for formatted table output
import warnings  # Import to manage warnings
from tqdm import tqdm  # Import for progress bars

# Attempt to import CuPy for GPU acceleration; fall back to NumPy if unavailable
try:
    import cupy as cp  # Import CuPy for GPU-accelerated computations
    cuda_available = True  # Set flag indicating CUDA is available
except ImportError:
    cuda_available = False  # Set flag indicating no CUDA support
    print("CUDA not available, falling back to CPU with NumPy.")  # Inform user of fallback
    import numpy as cp  # Use NumPy as a fallback for cp namespace

# Suppress specific UserWarnings from tabulate to avoid clutter
warnings.filterwarnings("ignore", category=UserWarning, module="tabulate")

# Configure logging to write to a file with a specific format
logging.basicConfig(
    filename='errors.log',  # Log file name
    level=logging.INFO,  # Log level set to INFO
    format='%(asctime)s [11_2mass_psc_validator.py] %(levelname)s: %(message)s'  # Log format with timestamp and script name
)

# Function to load configuration from a JSON file
def load_config():
    """
    Loads configuration settings from config_2mass.json or returns default values if loading fails.
    
    Returns:
        dict: Configuration dictionary containing analysis parameters.
    """
    try:
        with open('config_2mass.json', 'r', encoding='utf-8') as f:  # Open JSON file with UTF-8 encoding
            config = json.load(f)  # Parse JSON into a dictionary
            if 'expected_source_density' not in config:  # Check for missing key
                config['expected_source_density'] = 1.0  # Set default value if missing (sources/arcmin²)
            return config  # Return loaded configuration
    except Exception as e:  # Handle any errors during file loading
        logging.error(f"Failed to load config_2mass.json: {e}")  # Log the error
        return {  # Return default configuration if loading fails
            'targets': {'local_source_density': 1.0},  # Default target density (sources/arcmin²)
            'thresholds': {'local_source_density': 0.5},  # Default threshold for deviation
            'expected_source_density': 1.0,  # Default expected density
            'ra_bins': 36,  # Default number of RA bins
            'dec_bins': 18,  # Default number of DEC bins
            'min_count': 200  # Minimum count per bin, default 200
        }

# Function to update status with timestamped messages
def update_status(message):
    """
    Prints and logs a status message with a timestamp.
    
    Args:
        message (str): The message to display and log.
    """
    print(f"{datetime.now().strftime('%H:%M:%S')}: {message}")  # Print message with current time
    logging.info(message)  # Log the message

# Function to load CSV data
def load_csv_data():
    """Load data from results.csv into a dictionary."""
    data = {}
    if os.path.exists('results.csv'):
        with open('results.csv', 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            for row in reader:
                if len(row) >= 2:
                    script, param = row[0], row[1]
                    if script not in data:
                        data[script] = {}
                    data[script][param] = float(row[2]) if row[2] and row[2].replace('.', '').replace('-', '').replace('e', '').isdigit() else row[2]
    return data

# Function to parse 2MASS PSC file
def parse_psc_file(file_path):
    """
    Parses a 2MASS PSC ASCII file and extracts RA and Dec values using '|' delimiter.
    
    Args:
        file_path (str): Path to the PSC file.
    
    Returns:
        tuple: Arrays of RA, Dec values.
    """
    ra_vals, dec_vals = [], []
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            for line in tqdm(f, desc=f"Parsing {os.path.basename(file_path)}", unit="lines"):
                line = line.strip()
                if not line or line.startswith('#') or line.startswith('|'):
                    continue
                fields = line.split('|')
                if len(fields) >= 2:
                    try:
                        ra = float(fields[0])
                        dec = float(fields[1])
                        ra_vals.append(ra)
                        dec_vals.append(dec)
                    except ValueError:
                        continue  # skip invalid float conversion
        return np.array(ra_vals), np.array(dec_vals)
    except Exception as e:
        update_status(f"Error parsing {file_path}: {e}")
        return np.array([]), np.array([])


# Main analysis function
def run_analysis():
    """
    Executes the main analysis pipeline for 2MASS PSC file validation.
    Processes source data, performs sky binning, generates visualizations, and estimates structural consistency.
    """

    try:
        with open('results.csv', 'r', encoding='utf-8') as f:
            rows = list(csv.reader(f))
        rows = [row for row in rows if row and row[0] != '11_2mass_psc_validator.py']
        with open('results.csv', 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerows(rows)
    except FileNotFoundError:
        pass

    config = load_config()  # Load configuration settings
    ra_bins = config.get("ra_bins", 36)  # Get number of RA bins, default 36
    dec_bins = config.get("dec_bins", 18)  # Get number of DEC bins, default 18
    min_count = config.get("min_count", 200)  # Minimum count per bin, default 200

    # Collect data from all PSC files
    all_ra_vals, all_dec_vals = [], []
    psc_files = [f for f in os.listdir('.') if f.startswith('psc_') and os.path.isfile(f)]
    for psc_file in tqdm(psc_files, desc="Loading PSC files", unit="file"):
        ra_vals, dec_vals = parse_psc_file(psc_file)
        all_ra_vals.extend(ra_vals)
        all_dec_vals.extend(dec_vals)

    all_ra_vals = np.array(all_ra_vals)
    all_dec_vals = np.array(all_dec_vals)
    update_status(f"Loaded {len(all_ra_vals)} sources from {len(psc_files)} files.")

    if len(all_ra_vals) == 0:
        update_status("Failure: No valid source data.")
        return

    # Convert to CuPy array if CUDA is available
    ra_vals = cp.array(all_ra_vals) if cuda_available else all_ra_vals
    dec_vals = cp.array(all_dec_vals) if cuda_available else all_dec_vals

    # Sky binning section
    try:
        ra_edges = np.linspace(0, 360, ra_bins + 1)  # Define RA bin edges (0 to 360 degrees)
        dec_edges = np.linspace(-90, 90, dec_bins + 1)  # Define DEC bin edges (-90 to 90 degrees)
        results = []  # List to store binning results

        with tqdm(total=ra_bins * dec_bins, desc="Processing sky bins") as pbar_sky:
            for i in range(ra_bins):
                for j in range(dec_bins):
                    ra_min, ra_max = ra_edges[i], ra_edges[i + 1]  # Define RA range for current bin
                    dec_min, dec_max = dec_edges[j], dec_edges[j + 1]  # Define DEC range for current bin
                    mask = (ra_vals >= ra_min) & (ra_vals < ra_max) & (dec_vals >= dec_min) & (dec_vals < dec_max)  # Create mask for bin
                    region_count = cp.sum(mask).item() if cuda_available else np.sum(mask)  # Count data points in bin
                    deg2rad = np.pi / 180
                    dec1 = dec_min * deg2rad
                    dec2 = dec_max * deg2rad
                    ra_width = ra_max - ra_min

                    area_deg2 = ra_width * (np.sin(dec2) - np.sin(dec1))
                    area_arcmin2 = abs(area_deg2 * 3600)  # 1 deg² = 3600 arcmin²

                    mean_density = region_count / area_arcmin2 if region_count >= min_count and area_arcmin2 > 0 else np.nan

                    results.append([ra_min, ra_max, dec_min, dec_max, region_count, mean_density])
                    pbar_sky.update(1)

        # Min/Max Logging
        valid_densities = [r[5] for r in results if not np.isnan(r[5])]
        if valid_densities:
            update_status(f"Source Density Map: min={np.min(valid_densities):.3f}, max={np.max(valid_densities):.3f}")

        # Umrechnung von mean_density → mean_z (modellbasiert)
        # Annahme aus index.html: rho_expected ≈ 0.22 sources/arcmin² ↔ z ≈ 1.0
        rho_expected = 0.22
        output_path = "z_sky_mean_2mass.csv"
        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['ra_min', 'ra_max', 'dec_min', 'dec_max', 'count', 'mean_z', 'mean_density'])
            for row in results:
                ra_min, ra_max, dec_min, dec_max, count, mean_density = row
                if mean_density and not np.isnan(mean_density):
                    mean_z = mean_density / rho_expected
                else:
                    mean_z = ""
                    mean_density = ""
                writer.writerow([ra_min, ra_max, dec_min, dec_max, count, mean_z, mean_density])

        update_status(f"Sky binning results saved to: {output_path}")

    except Exception as e:
        update_status(f"Warning: Sky binning failed: {e}")
        return
    finally:
        del ra_vals, dec_vals  # Free memory

    # Density analysis
    valid_densities = [r[5] for r in results if not np.isnan(r[5])]
    if valid_densities:
        local_source_density = np.mean(valid_densities)
        update_status(f"Estimated Local Source Density: {local_source_density:.3f} (Expected: {config['expected_source_density']:.3f})")

        deviation = abs(local_source_density - config['expected_source_density'])
        test_passed = deviation <= config['thresholds']['local_source_density']
        status = "Success" if test_passed else "Failure"
        update_status(f"Test Result: {status} - Threshold: {config['thresholds']['local_source_density']:.3f}")

        results_path = "results.csv"
        script_id = "11_2mass_psc_validator.py"
        timestamp = datetime.now().strftime("%Y-%m-%dT%H:%M:%S")

        try:
            if os.path.exists(results_path):
                with open(results_path, "r", encoding="utf-8") as f:
                    reader = csv.reader(f)
                    rows = list(reader)
                header, *data_rows = rows
                data_rows = [row for row in data_rows if row[0] != script_id]
            else:
                header = ["script", "parameter", "value", "target", "deviation", "timestamp"]
                data_rows = []

            new_rows = [[
                script_id,
                "local_source_density",
                local_source_density,
                config['expected_source_density'],
                deviation,
                timestamp
            ]]

            with open(results_path, "w", newline="", encoding="utf-8") as f:
                writer = csv.writer(f)
                writer.writerow(header)
                writer.writerows(data_rows + new_rows)

            update_status(f"Results saved to: {results_path}")
        except Exception as e:
            update_status(f"Warning: Could not write to results.csv: {e}")

    # Histogram Heatmap
    plt.figure(figsize=(10, 6))
    plt.hist(valid_densities, bins=50, range=(0, max(valid_densities) if valid_densities else 2), color='blue', alpha=0.7)
    plt.title('Source Density Distribution')
    plt.xlabel('Density (sources/arcmin²)')
    plt.ylabel('Count')
    plt.grid(True)
    plt.savefig('img/11_source_density_heatmap.png')
    plt.close()
    update_status("Heatmap saved to: img/11_source_density_heatmap.png")

    # Optional: 10a–10e auf z_sky_mean_2mass.csv anwenden
    if os.path.exists("z_sky_mean_2mass.csv"):
        for script in [
            "10a_plot_z_sky_mean.py",
            "10b_neutrino_analysis.py",
            "10c_rg_entropy_flow.py",
            "10d_entropy_map.py",
            "10e_parameter_scan.py"
        ]:
            try:
                subprocess.run([sys.executable, script, "z_sky_mean_2mass.csv"], check=True)
                update_status(f"Executed {script} on 2MASS data.")
            except Exception as e:
                update_status(f"Warning: Failed to run {script} on 2MASS: {e}")

    update_status("2MASS PSC analysis completed.")

# Function to perform sky binning analysis
def perform_sky_bin_analysis(ra_vals, dec_vals, ra_bins, dec_bins, output_path, min_count=200, use_cuda=False):
    """
    Performs sky binning analysis on RA and Dec values, saving results to a CSV file.
    
    Args:
        ra_vals: Array of right ascension values.
        dec_vals: Array of declination values.
        ra_bins: Number of bins in RA direction.
        dec_bins: Number of bins in DEC direction.
        output_path: Path to save the output CSV file.
        min_count: Minimum number of data points per bin (default: 200).
        use_cuda: Flag to indicate if CUDA should be used (default: False).
    """
    try:
        if use_cuda:
            ra_vals, dec_vals = ra_vals.get(), dec_vals.get()
        ra_edges = np.linspace(0, 360, ra_bins + 1)
        dec_edges = np.linspace(-90, 90, dec_bins + 1)
        results = []

        for i in range(ra_bins):
            for j in range(dec_bins):
                ra_min, ra_max = ra_edges[i], ra_edges[i + 1]
                dec_min, dec_max = dec_edges[j], dec_edges[j + 1]
                mask = (ra_vals >= ra_min) & (ra_vals < ra_max) & (dec_vals >= dec_min) & (dec_vals < dec_max)
                region_count = np.sum(mask)
                mean_density = region_count / ((ra_max - ra_min) * (dec_max - dec_min) / 3600) if region_count >= min_count else np.nan
                results.append([ra_min, ra_max, dec_min, dec_max, region_count, mean_density])

        valid_densities = [r[5] for r in results if not np.isnan(r[5])]
        if valid_densities:
            update_status(f"Source Density Map: min={np.min(valid_densities):.3f}, max={np.max(valid_densities):.3f}")

        with open(output_path, 'w', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow(['ra_min', 'ra_max', 'dec_min', 'dec_max', 'count', 'mean_density'])
            writer.writerows(results)
        update_status(f"Sky binning results saved to: {output_path}")

    except Exception as e:
        update_status(f"Warning: Sky binning failed: {e}")

# Function to run a visualization script
def run_visualization_script(script_name):
    """
    Executes a specified visualization script using the system Python interpreter.
    
    Args:
        script_name (str): Name of the script to execute.
    """
    try:
        subprocess.run([sys.executable, script_name], check=True)
        update_status(f"Visualization script executed: {script_name}")
    except Exception as e:
        update_status(f"Warning: Could not run {script_name}: {e}")

if __name__ == "__main__":
    run_analysis()